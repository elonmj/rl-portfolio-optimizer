"""
ğŸš€ DEMO COMPLET - SAC Portfolio Optimizer
=======================================

Script de dÃ©monstration complÃ¨te qui :
1. EntraÃ®ne l'agent SAC sur 10 Ã©pisodes (test rapide)
2. Ã‰value les performances sur les pÃ©riodes de validation et test
3. Affiche les rÃ©sultats et gÃ©nÃ¨re les graphiques

Utilisation: python demo_complete.py
"""

import warnings
import logging
import os
import torch

from config import Config
from train import PortfolioTrainer
from evaluate_v2 import evaluate_model

# DÃ©sactiver les warnings inutiles
warnings.filterwarnings("ignore")

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

def run_complete_demo():
    """ExÃ©cute une dÃ©monstration complÃ¨te du systÃ¨me."""
    
    # Initialiser le device depuis la config
    device = Config.init_device()
    print(f"ğŸ”§ Device utilisÃ©: {device}")
    
    print("ğŸš€" + "="*60 + "ğŸš€")
    print("     SAC PORTFOLIO OPTIMIZER - DÃ‰MONSTRATION COMPLÃˆTE")
    print("ğŸš€" + "="*60 + "ğŸš€")
    print()
    print(f"âš™ï¸  Device utilisÃ© : {device}")
    print()
    
    # ========================================
    # PHASE 1: ENTRAÃNEMENT RAPIDE
    # ========================================
    print("ğŸ“ˆ PHASE 1: ENTRAÃNEMENT DE L'AGENT SAC")
    print("-" * 50)
    
    config_overrides = {
        "DEVICE": device,              # <-- correction pour forcer le bon device
        "EVAL_FREQUENCY": 5,
        "SAVE_FREQUENCY": 20,
        "BATCH_SIZE": 128,
        "MAX_STEPS_PER_EPISODE": 100,  # RÃ©duit pour test rapide
    }
    
    model_path = "models/sac_portfolio_agent.pth"
    
    try:
        trainer = PortfolioTrainer(config_overrides)
        print("ğŸ§  DÃ©marrage de l'entraÃ®nement (5 Ã©pisodes)...")
        
        metrics = trainer.train(num_episodes=5)
        
        print("âœ… EntraÃ®nement terminÃ© avec succÃ¨s!")
        if metrics and "total_return" in metrics and metrics["total_return"]:
            final_return = metrics["total_return"][-1]
            print(f"ğŸ“Š Retour final d'entraÃ®nement: {final_return:.2%}")
        
        print(f"ğŸ’¾ ModÃ¨le sauvegardÃ© dans: {model_path}")
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'entraÃ®nement: {e}")
        print("âš ï¸  Continuons avec l'Ã©valuation d'un modÃ¨le non-entraÃ®nÃ©...")
    
    print()
    
    # ========================================
    # PHASE 2: Ã‰VALUATION COMPLÃˆTE
    # ========================================
    print("ğŸ“Š PHASE 2: Ã‰VALUATION DES PERFORMANCES")
    print("-" * 50)
    
    try:
        print("ğŸ” Ã‰valuation sur les pÃ©riodes validation et test...")
        
        # VÃ©rifier que le modÃ¨le existe
        eval_model_path = "models/final_model.pth"
        
        results = evaluate_model(model_path=eval_model_path)
        
        if results:
            print("âœ… Ã‰valuation terminÃ©e avec succÃ¨s!")
            print()
            print("ğŸ“ˆ RÃ‰SULTATS OBTENUS:")
            print("=" * 30)
            
            for period, metrics in results.items():
                if "Agent" in period:
                    period_name = period.replace("Agent_", "")
                    print(f"\nğŸ¯ PÃ©riode {period_name}:")
                    print(f"   â€¢ Rendement total: {metrics.get('total_return', 0):.2%}")
                    print(f"   â€¢ Rendement annualisÃ©: {metrics.get('annualized_return', 0):.2%}")
                    print(f"   â€¢ Ratio de Sharpe: {metrics.get('sharpe_ratio', 0):.3f}")
                    print(f"   â€¢ VolatilitÃ©: {metrics.get('volatility', 0):.2%}")
                    print(f"   â€¢ Max Drawdown: {metrics.get('max_drawdown', 0):.2%}")
            
            print(f"\nğŸ“Š Graphiques sauvegardÃ©s dans: results/")
            print(f"ğŸ“‹ MÃ©triques dÃ©taillÃ©es dans: results/metrics_summary.csv")
            
        else:
            print("âš ï¸  Ã‰valuation n'a pas retournÃ© de rÃ©sultats")
            
    except Exception as e:
        print(f"âŒ Erreur lors de l'Ã©valuation: {e}")
        import traceback
        traceback.print_exc()
    
    print()
    
    # ========================================
    # CONCLUSION
    # ========================================
    print("ğŸ‰" + "="*60 + "ğŸ‰")
    print("               DÃ‰MONSTRATION TERMINÃ‰E")
    print("ğŸ‰" + "="*60 + "ğŸ‰")
    print()
    print("ğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
    print(f"   â€¢ {model_path} - ModÃ¨le entraÃ®nÃ©")
    print("   â€¢ results/performance_analysis.png - Graphiques de performance")
    print("   â€¢ results/metrics_summary.csv - MÃ©triques dÃ©taillÃ©es")
    print("   â€¢ logs/ - Logs d'entraÃ®nement")
    print()
    print("ğŸ”„ Pour un entraÃ®nement complet (1000 Ã©pisodes):")
    print("   python train.py")
    print()
    print("ğŸ“Š Pour Ã©valuation uniquement:")
    print("   python evaluate_v2.py")
    print()

if __name__ == "__main__":
    run_complete_demo()
