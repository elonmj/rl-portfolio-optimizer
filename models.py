"""
Mod√®les de r√©seaux de neurones pour l'agent SAC simplifi√©.
Impl√©mente Actor et Critic sans m√©canisme d'attention pour performances optimales.
Version compatible avec modelisation.pdf Section 2.5.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, Any
import math

from config import Config


class SimpleActor(nn.Module):
    """
    R√©seau Actor simplifi√© pour SAC selon modelisation.pdf Section 2.5.
    Architecture efficace sans attention pour performances optimales.
    """
    
    def __init__(self, 
                 state_dim: int,
                 action_dim: int,
                 hidden_dim: int = 256):
        super().__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # R√©seau principal
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # T√™tes pour moyenne et log-variance
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)
        
        # Contraintes pour log_std
        self.log_std_min = -20
        self.log_std_max = 2
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass retournant moyenne et log-std pour chaque action
        """
        features = self.network(state)
        
        mean = self.mean_head(features)
        log_std = self.log_std_head(features)
        
        # Contraindre log_std dans une plage raisonnable
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        
        return mean, log_std
    
    def sample(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        √âchantillonnage avec reparameterization trick (√âquation 18)
        """
        mean, log_std = self.forward(state)
        
        if deterministic:
            action = torch.tanh(mean)
            log_prob = None
        else:
            std = log_std.exp()
            
            # Reparameterization trick
            normal = torch.distributions.Normal(mean, std)
            x_t = normal.rsample()  # rsample pour gradient
            
            # Transformation tanh (√âquation 18)
            action = torch.tanh(x_t)
            
            # Calcul log-probabilit√© avec correction Jacobienne
            log_prob = normal.log_prob(x_t)
            log_prob -= torch.log(1 - action.pow(2) + 1e-6)
            log_prob = log_prob.sum(-1, keepdim=True)
        
        return action, log_prob


class SimpleCritic(nn.Module):
    """
    R√©seau Critic simplifi√© pour SAC selon modelisation.pdf Section 2.5.
    Impl√©mente Q(s,a) avec architecture efficace.
    """
    
    def __init__(self, 
                 state_dim: int, 
                 action_dim: int, 
                 hidden_dim: int = 256):
        super().__init__()
        
        # R√©seau Q-value
        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(), 
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Forward pass Q(s,a)
        """
        x = torch.cat([state, action], dim=1)
        return self.network(x)


# Alias pour compatibilit√© avec le code existant
class ActorWithAttention(SimpleActor):
    """
    Wrapper de compatibilit√© - utilise maintenant SimpleActor sans attention
    Remplace l'ancienne impl√©mentation complexe avec attention
    """
    
    def __init__(self, 
                 num_assets: int,
                 feature_dim: int = Config.FEATURE_DIM,
                 hidden_dim: int = Config.HIDDEN_DIM,
                 attention_heads: int = Config.ATTENTION_HEADS):
        
        # Calculer la dimension d'√©tat selon l'espace d'√©tat am√©lior√©
        state_dim = 339  # Dimension de l'espace d'√©tat am√©lior√©
        action_dim = num_assets
        
        super().__init__(state_dim, action_dim, hidden_dim)
        
        print(f"üìä SAC Actor simplifi√© initialis√©: {state_dim}‚Üí{action_dim} (sans attention)")


class CriticWithAttention(SimpleCritic):
    """
    Wrapper de compatibilit√© - utilise maintenant SimpleCritic sans attention
    Remplace l'ancienne impl√©mentation complexe avec attention
    """
    
    def __init__(self, 
                 num_assets: int,
                 feature_dim: int = Config.FEATURE_DIM,
                 hidden_dim: int = Config.HIDDEN_DIM,
                 attention_heads: int = Config.ATTENTION_HEADS):
        
        # Calculer les dimensions
        state_dim = 339  # Dimension de l'espace d'√©tat am√©lior√©
        action_dim = num_assets
        
        super().__init__(state_dim, action_dim, hidden_dim)
        
        print(f"üìä SAC Critic simplifi√© initialis√©: {state_dim}+{action_dim}‚Üí1 (sans attention)")


# Fonction utilitaire pour cr√©er les mod√®les
def create_sac_models(num_assets: int, device: torch.device = None):
    """
    Cr√©e les mod√®les SAC simplifi√©s pour l'entra√Ænement
    """
    device = device or torch.device('cpu')
    
    # Actor
    actor = ActorWithAttention(num_assets=num_assets)
    actor.to(device)
    
    # Critics (double critic pour stabilit√©)
    critic1 = CriticWithAttention(num_assets=num_assets) 
    critic2 = CriticWithAttention(num_assets=num_assets)
    critic1.to(device)
    critic2.to(device)
    
    # Target critics
    target_critic1 = CriticWithAttention(num_assets=num_assets)
    target_critic2 = CriticWithAttention(num_assets=num_assets)
    target_critic1.load_state_dict(critic1.state_dict())
    target_critic2.load_state_dict(critic2.state_dict())
    target_critic1.to(device)
    target_critic2.to(device)
    
    return {
        'actor': actor,
        'critic1': critic1,
        'critic2': critic2, 
        'target_critic1': target_critic1,
        'target_critic2': target_critic2
    }